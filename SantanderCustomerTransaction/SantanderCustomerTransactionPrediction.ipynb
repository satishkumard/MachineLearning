{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the test file\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import lightgbm as lgb \n",
    "# import xgboost as xgb \n",
    "\n",
    "#Loading the dataset\n",
    "trainDF = pd.read_csv('santander_train.csv')\n",
    "testDF = pd.read_csv('santander_test.csv')\n",
    "\n",
    "X = trainDF.iloc[:,2:].values\n",
    "y = trainDF.iloc[:,1].values\n",
    "\n",
    "# Actual test data\n",
    "X_pred_test = testDF.iloc[:,1:].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any null values in test file\n",
    "# for row in range(len(X_pred_test[0])):\n",
    "#     for col in range(len(X_pred_test[0])):\n",
    "#         if np.isnan(X_pred_test[row,col]) == True:\n",
    "#             print('Null')\n",
    "\n",
    "print(trainDF.isnull().values.any())\n",
    "print(testDF.isnull().values.any())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Preparation Steps using backward elimination\n",
    "# We will now be importing some required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#Loading the dataset\n",
    "trainDF = pd.read_csv('santander_train.csv')\n",
    "testDF = pd.read_csv('santander_test.csv')\n",
    "\n",
    "X = trainDF.iloc[:,2:].values\n",
    "y = trainDF.iloc[:,1].values\n",
    "\n",
    "# Backward Elimination with p-values only:\n",
    "# import statsmodels.formula.api as sm\n",
    "# X = np.append ( arr = np.ones([200000,1]).astype(int), values = X, axis = 1)\n",
    "# def backwardElimination(x, sl):\n",
    "#     numVars = len(x[0])\n",
    "#     for i in range(0, numVars):\n",
    "#         obj_OLS = sm.OLS(endog=y, exog=x).fit()\n",
    "#         maxVar = max(obj_OLS.pvalues).astype(float)\n",
    "#         if maxVar > sl:\n",
    "#             for j in range(0, numVars - i):\n",
    "#                 if (obj_OLS.pvalues[j].astype(float) == maxVar):\n",
    "#                     x = np.delete(x, j, 1)\n",
    "#     obj_OLS.summary()\n",
    "#     return x\n",
    " \n",
    "# SL = 0.05\n",
    "# X_opt = X[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199]]\n",
    "# X_Modeled = backwardElimination(X_opt, SL)\n",
    "\n",
    "### Model selection training using the entire train.csv and run the test.csv to run against the model\n",
    "\n",
    "# Actual test data\n",
    "X_pred_test = testDF.iloc[:,1:].values\n",
    "\n",
    "# Applying the feature selection on test data\n",
    "X_pred_test= X_pred_test[:, [0,1,2,3,4,5,6,7,9,10,12,13,14,15,16,17,19,20,21,22,23,24,25,26,28,29,31,32,33,35,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,92,93,95,97,98,99,100,101,102,103,104,105,106,107,109,110,111,113,114,115,116,117,119,120,121,122,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,144,145,146,147,148,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,171,172,173,174,175,176,177,178,179,180,181,182,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199]]\n",
    "\n",
    "                     \n",
    "# Normalizing the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_Modeled)\n",
    "X_pred_test = sc_X.transform(X_pred_test)\n",
    "\n",
    "\n",
    "# Multiple Linear Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifierObj = LogisticRegression()\n",
    "classifierObj.fit(X_train, y)\n",
    "\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_test = classifierObj.predict(X_pred_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using logistic regression Model\n",
    "\n",
    "\n",
    "### Data Preparation Steps \n",
    "# We will now be importing some required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#Loading the dataset\n",
    "trainDF = pd.read_csv('santander_train.csv')\n",
    "testDF = pd.read_csv('santander_test.csv')\n",
    "\n",
    "X = trainDF.iloc[:,2:].values\n",
    "y = trainDF.iloc[:,1].values\n",
    "\n",
    "# Actual test data\n",
    "X_pred_test = testDF.iloc[:,1:].values\n",
    "\n",
    "#Splitting the data into Training Set and Test Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n",
    "\n",
    "\n",
    "#Normalizing the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "X_pred_test = sc_X.transform(X_pred_test)\n",
    "\n",
    "# # Applying PCA\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components = 2)\n",
    "# X_train = pca.fit_transform(X_train)\n",
    "# X_test = pca.transform(X_test)\n",
    "# X_pred_test = pca.transform(X_pred_test)\n",
    "# explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Applying LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components = 150)\n",
    "X_train = lda.fit_transform(X_train, y_train)\n",
    "X_test = lda.transform(X_test)\n",
    "X_pred_test = lda.transform(X_pred_test)\n",
    "\n",
    "# Applying Kernel PCA\n",
    "# from sklearn.decomposition import KernelPCA\n",
    "# kpca = KernelPCA(n_components = 2, kernel = 'rbf')\n",
    "# X_train = kpca.fit_transform(X_train)\n",
    "# X_test = kpca.transform(X_test)\n",
    "# X_pred_test = kpca.transform(X_pred_test)\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifierObj = LogisticRegression()\n",
    "classifierObj.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifierObj.predict(X_test)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_test = classifierObj.predict(X_pred_test)\n",
    "\n",
    "# Evaluating the predictions using a Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "print(cm)\n",
    "print(100-(cm[0,1] + cm[1,0])/(cm[0,0]+cm[1,1]) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(explained_variance)):\n",
    "    print(explained_variance[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Decision Tree Model\n",
    "\n",
    "\n",
    "### Data Preparation Steps \n",
    "# We will now be importing some required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#Loading the dataset\n",
    "trainDF = pd.read_csv('santander_train.csv')\n",
    "testDF = pd.read_csv('santander_test.csv')\n",
    "\n",
    "X = trainDF.iloc[:,2:].values\n",
    "y = trainDF.iloc[:,1].values\n",
    "\n",
    "# Actual test data\n",
    "X_pred_test = testDF.iloc[:,1:].values\n",
    "\n",
    "#Splitting the data into Training Set and Test Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n",
    "\n",
    "\n",
    "#Normalizing the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "X_pred_test = sc_X.transform(X_pred_test)\n",
    "\n",
    "# # Applying PCA\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components = 2)\n",
    "# X_train = pca.fit_transform(X_train)\n",
    "# X_test = pca.transform(X_test)\n",
    "# X_pred_test = pca.transform(X_pred_test)\n",
    "# explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# # Applying LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components = 200)\n",
    "X_train = lda.fit_transform(X_train, y_train)\n",
    "X_test = lda.transform(X_test)\n",
    "X_pred_test = lda.transform(X_pred_test)\n",
    "\n",
    "# Applying Kernel PCA\n",
    "# from sklearn.decomposition import KernelPCA\n",
    "# kpca = KernelPCA(n_components = 2, kernel = 'rbf')\n",
    "# X_train = kpca.fit_transform(X_train)\n",
    "# X_test = kpca.transform(X_test)\n",
    "# X_pred_test = kpca.transform(X_pred_test)\n",
    "\n",
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifierObj = DecisionTreeClassifier('entropy')\n",
    "classifierObj.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifierObj.predict(X_test)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_test = classifierObj.predict(X_pred_test)\n",
    "\n",
    "# Evaluating the predictions using a Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "print(cm)\n",
    "print(100-(cm[0,1] + cm[1,0])/(cm[0,0]+cm[1,1]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Random Forest algorithm\n",
    "\n",
    "### Data Preparation Steps \n",
    "# We will now be importing some required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#Loading the dataset\n",
    "trainDF = pd.read_csv('santander_train.csv')\n",
    "testDF = pd.read_csv('santander_test.csv')\n",
    "\n",
    "X = trainDF.iloc[:,2:].values\n",
    "y = trainDF.iloc[:,1].values\n",
    "\n",
    "# Actual test data\n",
    "X_pred_test = testDF.iloc[:,1:].values\n",
    "\n",
    "#Splitting the data into Training Set and Test Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n",
    "\n",
    "#Normalizing the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "X_pred_test = sc_X.transform(X_pred_test)\n",
    "\n",
    "# Applying PCA\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components = 2)\n",
    "# X_train = pca.fit_transform(X_train)\n",
    "# X_test = pca.transform(X_test)\n",
    "# X_pred_test = pca.transform(X_pred_test)\n",
    "# explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# # Applying LDA\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "# lda = LDA(n_components = 200)\n",
    "# X_train = lda.fit_transform(X_train, y_train)\n",
    "# X_test = lda.transform(X_test)\n",
    "# X_pred_test = lda.transform(X_pred_test)\n",
    "\n",
    "# Applying Kernel PCA\n",
    "# from sklearn.decomposition import KernelPCA\n",
    "# kpca = KernelPCA(n_components = 2, kernel = 'rbf')\n",
    "# X_train = kpca.fit_transform(X_train)\n",
    "# X_test = kpca.transform(X_test)\n",
    "# X_pred_test = kpca.transform(X_pred_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "score =[]\n",
    "for n in range(1,200,10):\n",
    "    classifierObj = RandomForestClassifier(n_estimators=100,criterion='entropy')\n",
    "    classifierObj.fit(X_train, y_train)\n",
    "\n",
    "    # Accuracy of the model\n",
    "    rfScore = classifierObj.score(X_test, y_test)\n",
    "    score.append([n,rfScore]) \n",
    "    \n",
    "scoreDF = pd.DataFrame(data=score, columns = ['n_estimators', 'Score'])\n",
    "\n",
    "# Plotting Degree Vs Accuracy Score\n",
    "xticks = [i for i in range(0,200,10)]\n",
    "plt.plot(scoreDF.n_estimators,scoreDF.Score,color='red')\n",
    "plt.title('Random Forest - n_estimators Vs Accuracy Score')\n",
    "plt.xlabel('n estimators')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(ticks=xticks)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(scoreDF[scoreDF.Score==max(scoreDF.Score)])\n",
    "\n",
    "\n",
    "# LDA 261  0.865417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Naive Bayes Algorithm\n",
    "\n",
    "### Data Preparation Steps \n",
    "# We will now be importing some required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Loading the dataset\n",
    "trainDF = pd.read_csv('santander_train.csv')\n",
    "testDF = pd.read_csv('santander_test.csv')\n",
    "\n",
    "X = trainDF.iloc[:,2:].values\n",
    "y = trainDF.iloc[:,1].values\n",
    "\n",
    "# X= X[:, [0,1,2,3,4,5,6,7,9,10,12,13,14,15,16,17,19,20,21,22,23,24,25,26,28,29,31,32,33,35,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,92,93,95,97,98,99,100,101,102,103,104,105,106,107,109,110,111,113,114,115,116,117,119,120,121,122,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,144,145,146,147,148,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,171,172,173,174,175,176,177,178,179,180,181,182,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199]]\n",
    "\n",
    "# Actual test data\n",
    "X_pred_te = testDF.iloc[:,1:].values\n",
    "\n",
    "# X_pred_te= X_pred_te[:, [0,1,2,3,4,5,6,7,9,10,12,13,14,15,16,17,19,20,21,22,23,24,25,26,28,29,31,32,33,35,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,92,93,95,97,98,99,100,101,102,103,104,105,106,107,109,110,111,113,114,115,116,117,119,120,121,122,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,144,145,146,147,148,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,171,172,173,174,175,176,177,178,179,180,181,182,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199]]\n",
    "\n",
    "# Splitting the data into Training Set and Test Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X,y,test_size=0.3)\n",
    "\n",
    "# Normalizing the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_tr = sc_X.fit_transform(X_tr)\n",
    "X_te = sc_X.transform(X_te)\n",
    "X_pred_te = sc_X.transform(X_pred_te)\n",
    "\n",
    "n=200\n",
    "# Applying PCA\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components = 200)\n",
    "# X_train = pca.fit_transform(X_tr)\n",
    "# X_test = pca.transform(X_te)\n",
    "# X_pred_test = pca.transform(X_pred_te)\n",
    "# explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "\n",
    "\n",
    "# # Applying LDA\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "# lda = LDA(n_components = 200)\n",
    "# X_train = lda.fit_transform(X_tr, y_tr)\n",
    "# X_test = lda.transform(X_te)\n",
    "# X_pred_test = lda.transform(X_pred_te)\n",
    "\n",
    "# Applying Kernel PCA\n",
    "# from sklearn.decomposition import KernelPCA\n",
    "# kpca = KernelPCA(n_components = 2, kernel = 'rbf')\n",
    "# X_train = kpca.fit_transform(X_tr)\n",
    "# X_test = kpca.transform(X_te)\n",
    "# X_pred_test = kpca.transform(X_pred_te)\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifierObj = GaussianNB()\n",
    "classifierObj.fit(X_tr, y_tr)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifierObj.predict(X_te)\n",
    "\n",
    "# Evaluating the predictions using a Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_te,y_pred)\n",
    "\n",
    "y_pred_test = classifierObj.predict(X_pred_te)\n",
    "\n",
    "print(n)\n",
    "print(cm)\n",
    "print(100-(cm[0,1] + cm[1,0])/(cm[0,0]+cm[1,1]) * 100)\n",
    "print(classifierObj.score(X_te, y_te))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using K-NN algorithm\n",
    "\n",
    "\n",
    "### Data Preparation Steps \n",
    "# We will now be importing some required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#Loading the dataset\n",
    "trainDF = pd.read_csv('santander_train.csv')\n",
    "testDF = pd.read_csv('santander_test.csv')\n",
    "\n",
    "X = trainDF.iloc[:,2:].values\n",
    "y = trainDF.iloc[:,1].values\n",
    "\n",
    "# Actual test data\n",
    "X_pred_test = testDF.iloc[:,1:].values\n",
    "\n",
    "#Splitting the data into Training Set and Test Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.025)\n",
    "\n",
    "# Normalizing the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "X_pred_test = sc_X.transform(X_pred_test)\n",
    "\n",
    "# Applying PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "X_pred_test = pca.transform(X_pred_test)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Applying LDA\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "# lda = LDA(n_components = 2)\n",
    "# X_train = lda.fit_transform(X_train, y_train)\n",
    "# X_test = lda.transform(X_test)\n",
    "# X_pred_test = lda.transform(X_pred_test)\n",
    "\n",
    "# Applying Kernel PCA\n",
    "# from sklearn.decomposition import KernelPCA\n",
    "# kpca = KernelPCA(n_components = 2, kernel = 'rbf')\n",
    "# X_train = kpca.fit_transform(X_train)\n",
    "# X_test = kpca.transform(X_test)\n",
    "# X_pred_test = kpca.transform(X_pred_test)\n",
    "\n",
    "# K-NN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifierObj = KNeighborsClassifier(n_neighbors=10)\n",
    "classifierObj.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifierObj.predict(X_test)\n",
    "\n",
    "# Evaluating the predictions using a Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "y_pred_test = classifierObj.predict(X_pred_test)\n",
    "\n",
    "print(cm)\n",
    "print(100-(cm[0,1] + cm[1,0])/(cm[0,0]+cm[1,1]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel SVM \n",
    "\n",
    "### Data Preparation Steps \n",
    "# We will now be importing some required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#Loading the dataset\n",
    "trainDF = pd.read_csv('santander_train.csv')\n",
    "testDF = pd.read_csv('santander_test.csv')\n",
    "\n",
    "X = trainDF.iloc[:,2:].values\n",
    "y = trainDF.iloc[:,1].values\n",
    "\n",
    "# Actual test data\n",
    "X_pred_test = testDF.iloc[:,1:].values\n",
    "\n",
    "#Splitting the data into Training Set and Test Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.025)\n",
    "\n",
    "#Normalizing the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "X_pred_test = sc_X.transform(X_pred_test)\n",
    "\n",
    "# # Applying PCA\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components = 200)\n",
    "# X_train = pca.fit_transform(X_train)\n",
    "# X_test = pca.transform(X_test)\n",
    "# X_pred_test = pca.transform(X_pred_test)\n",
    "# explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# # Applying LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components = 200)\n",
    "X_train = lda.fit_transform(X_train, y_train)\n",
    "X_test = lda.transform(X_test)\n",
    "X_pred_test = lda.transform(X_pred_test)\n",
    "\n",
    "# Applying Kernel PCA\n",
    "# from sklearn.decomposition import KernelPCA\n",
    "# kpca = KernelPCA(n_components = 2, kernel = 'rbf')\n",
    "# X_train = kpca.fit_transform(X_train)\n",
    "# X_test = kpca.transform(X_test)\n",
    "# X_pred_test = kpca.transform(X_pred_test)\n",
    "\n",
    "# Kernel SVM\n",
    "from sklearn.svm import SVC\n",
    "classifierObj = SVC()\n",
    "classifierObj.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifierObj.predict(X_test)\n",
    "\n",
    "# Evaluating the predictions using a Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "y_pred_test = classifierObj.predict(X_pred_test)\n",
    "\n",
    "print(cm)\n",
    "print(100-(cm[0,1] + cm[1,0])/(cm[0,0]+cm[1,1]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using lightgbm \n",
    "# Pre-processing\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import lightgbm as lgb \n",
    "\n",
    "ver = 'lgbm_v5'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "#Loading the dataset\n",
    "trainDF = pd.read_csv('santander_train.csv')\n",
    "testDF = pd.read_csv('santander_test.csv')\n",
    "\n",
    "X = trainDF.iloc[:,2:].values\n",
    "y = trainDF.iloc[:,1].values\n",
    "\n",
    "test = testDF.iloc[:,1:].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20000 rounds.\n",
      "[20000]\ttraining's auc: 0.999999\tvalid_1's auc: 0.895198\n",
      "Early stopping, best iteration is:\n",
      "[4154]\ttraining's auc: 0.99722\tvalid_1's auc: 0.896348\n",
      "[0.0957652  0.20755523 0.12423252 ... 0.00659181 0.10761683 0.05958481]\n",
      "Training until validation scores don't improve for 20000 rounds.\n",
      "[20000]\ttraining's auc: 0.999997\tvalid_1's auc: 0.898862\n",
      "Early stopping, best iteration is:\n",
      "[4870]\ttraining's auc: 0.998768\tvalid_1's auc: 0.900289\n",
      "[0.05817816 0.23143801 0.14075543 ... 0.006539   0.07554377 0.06412677]\n",
      "Training until validation scores don't improve for 20000 rounds.\n",
      "[20000]\ttraining's auc: 1\tvalid_1's auc: 0.894953\n",
      "Early stopping, best iteration is:\n",
      "[3073]\ttraining's auc: 0.993007\tvalid_1's auc: 0.896193\n",
      "[0.1325709  0.18642596 0.11071342 ... 0.0121496  0.10208391 0.04958229]\n",
      "Training until validation scores don't improve for 20000 rounds.\n",
      "[20000]\ttraining's auc: 1\tvalid_1's auc: 0.898019\n",
      "Early stopping, best iteration is:\n",
      "[3686]\ttraining's auc: 0.995834\tvalid_1's auc: 0.899726\n",
      "[0.11593154 0.18098937 0.18635725 ... 0.00748391 0.08824742 0.06511901]\n",
      "Training until validation scores don't improve for 20000 rounds.\n",
      "[20000]\ttraining's auc: 0.999997\tvalid_1's auc: 0.89734\n",
      "Early stopping, best iteration is:\n",
      "[3989]\ttraining's auc: 0.996767\tvalid_1's auc: 0.899244\n",
      "[0.08969096 0.19296689 0.12915106 ... 0.01008291 0.07309433 0.05305336]\n",
      "Training until validation scores don't improve for 20000 rounds.\n",
      "[20000]\ttraining's auc: 0.999994\tvalid_1's auc: 0.897736\n",
      "Early stopping, best iteration is:\n",
      "[4249]\ttraining's auc: 0.997551\tvalid_1's auc: 0.899358\n",
      "[0.07666616 0.14035163 0.10670753 ... 0.00529164 0.11953719 0.05989179]\n",
      "Training until validation scores don't improve for 20000 rounds.\n",
      "[20000]\ttraining's auc: 1\tvalid_1's auc: 0.896652\n",
      "Early stopping, best iteration is:\n",
      "[4376]\ttraining's auc: 0.997833\tvalid_1's auc: 0.898347\n",
      "[0.0748058  0.15100791 0.16915313 ... 0.00808632 0.09970702 0.0829566 ]\n",
      "Training until validation scores don't improve for 20000 rounds.\n",
      "[20000]\ttraining's auc: 0.999988\tvalid_1's auc: 0.894616\n",
      "Early stopping, best iteration is:\n",
      "[2926]\ttraining's auc: 0.992339\tvalid_1's auc: 0.896382\n",
      "[0.09261049 0.1762388  0.15612697 ... 0.01775108 0.11462556 0.06277917]\n",
      "Training until validation scores don't improve for 20000 rounds.\n",
      "[20000]\ttraining's auc: 0.999978\tvalid_1's auc: 0.895769\n",
      "Early stopping, best iteration is:\n",
      "[3881]\ttraining's auc: 0.996568\tvalid_1's auc: 0.897518\n",
      "[0.07868146 0.14222104 0.14847678 ... 0.00917331 0.06774747 0.08412238]\n",
      "Training until validation scores don't improve for 20000 rounds.\n",
      "[20000]\ttraining's auc: 1\tvalid_1's auc: 0.896584\n",
      "Early stopping, best iteration is:\n",
      "[3884]\ttraining's auc: 0.996556\tvalid_1's auc: 0.898499\n",
      "[0.08487648 0.22053125 0.14838046 ... 0.00958081 0.11340255 0.08478431]\n",
      "Training until validation scores don't improve for 20000 rounds.\n",
      "[20000]\ttraining's auc: 1\tvalid_1's auc: 0.899224\n",
      "Early stopping, best iteration is:\n",
      "[3579]\ttraining's auc: 0.995268\tvalid_1's auc: 0.901093\n",
      "[0.09950065 0.15089623 0.12081719 ... 0.0126058  0.09703955 0.06548709]\n",
      "Training until validation scores don't improve for 20000 rounds.\n",
      "[20000]\ttraining's auc: 0.999992\tvalid_1's auc: 0.897566\n",
      "Early stopping, best iteration is:\n",
      "[3743]\ttraining's auc: 0.995954\tvalid_1's auc: 0.899392\n",
      "[0.09299659 0.14261733 0.15845941 ... 0.00862909 0.06236729 0.05078063]\n",
      "Training until validation scores don't improve for 20000 rounds.\n",
      "[20000]\ttraining's auc: 1\tvalid_1's auc: 0.89315\n",
      "Early stopping, best iteration is:\n",
      "[3912]\ttraining's auc: 0.996668\tvalid_1's auc: 0.894813\n",
      "[0.06675033 0.12632194 0.11194537 ... 0.00935914 0.12943168 0.07189813]\n",
      "Training until validation scores don't improve for 20000 rounds.\n",
      "[20000]\ttraining's auc: 1\tvalid_1's auc: 0.898088\n",
      "Early stopping, best iteration is:\n",
      "[4177]\ttraining's auc: 0.997421\tvalid_1's auc: 0.899426\n",
      "[0.118625   0.17047999 0.18438623 ... 0.01197762 0.09699024 0.05328215]\n",
      "Training until validation scores don't improve for 20000 rounds.\n",
      "[20000]\ttraining's auc: 1\tvalid_1's auc: 0.894788\n",
      "Early stopping, best iteration is:\n",
      "[4348]\ttraining's auc: 0.997848\tvalid_1's auc: 0.89659\n",
      "[0.07357826 0.15283917 0.13823696 ... 0.00785672 0.10334906 0.06090233]\n",
      "Training until validation scores don't improve for 20000 rounds.\n",
      "[20000]\ttraining's auc: 1\tvalid_1's auc: 0.896344\n",
      "Early stopping, best iteration is:\n",
      "[3946]\ttraining's auc: 0.99668\tvalid_1's auc: 0.897226\n",
      "[0.08183907 0.16818341 0.15608728 ... 0.00794608 0.05618249 0.06695276]\n",
      "Training until validation scores don't improve for 20000 rounds.\n",
      "[20000]\ttraining's auc: 0.999996\tvalid_1's auc: 0.894713\n",
      "Early stopping, best iteration is:\n",
      "[5767]\ttraining's auc: 0.99958\tvalid_1's auc: 0.896485\n",
      "[0.06994516 0.18855536 0.14878567 ... 0.00429648 0.06465946 0.0412325 ]\n",
      "Training until validation scores don't improve for 20000 rounds.\n",
      "[20000]\ttraining's auc: 1\tvalid_1's auc: 0.895768\n",
      "Early stopping, best iteration is:\n",
      "[4157]\ttraining's auc: 0.997318\tvalid_1's auc: 0.897084\n",
      "[0.08583937 0.14499436 0.10189605 ... 0.00870954 0.09277173 0.0406298 ]\n",
      "Training until validation scores don't improve for 20000 rounds.\n",
      "[20000]\ttraining's auc: 1\tvalid_1's auc: 0.895314\n",
      "Early stopping, best iteration is:\n",
      "[3455]\ttraining's auc: 0.99482\tvalid_1's auc: 0.896646\n",
      "[0.12719586 0.17668401 0.16458072 ... 0.01039623 0.08928422 0.0754753 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred = pd.DataFrame()\n",
    "for i in range (1, 20):\n",
    "    #Splitting the data into Training Set and Test Set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n",
    "    \n",
    "    # Using lightgbm \n",
    "\n",
    "    d_train = lgb.Dataset(X_train, label=y_train)\n",
    "    d_test = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "    params = {\n",
    "        'bagging_freq': 5,\n",
    "        'bagging_fraction': 0.4,\n",
    "        'boost_from_average':'true',\n",
    "        'boost': 'gbdt',\n",
    "        'feature_fraction': 0.05,\n",
    "        'learning_rate': 0.0083,\n",
    "        'max_depth': -1,  \n",
    "        'metric':'auc',\n",
    "        'min_data_in_leaf': 80,\n",
    "        'min_sum_hessian_in_leaf': 10.0,\n",
    "        'num_leaves': 100,\n",
    "        'num_threads': 8,\n",
    "        'tree_learner': 'serial',\n",
    "        'objective': 'binary', \n",
    "        'verbosity': 1,\n",
    "        'seed': i,\n",
    "#         'feature_fraction_seed': i,\n",
    "#         'bagging_seed': i,\n",
    "#         'drop_seed': i,\n",
    "#         'data_random_seed': i,\n",
    "    }\n",
    "\n",
    "    clf = lgb.train(params, d_train, 200000, valid_sets = [d_train, d_test], verbose_eval=20000, early_stopping_rounds = 20000)\n",
    "\n",
    "    #Prediction\n",
    "    pr=clf.predict(test, num_iteration=clf.best_iteration)\n",
    "    print(pr)\n",
    "    pred[i] = pr\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "ver = 'lgbm_v5'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "submission_ = pd.read_csv('C:/Users/satishkumard/Google Drive/DataScience/MachineLearningProjects/SantanderCustomerTransaction/santander_submission1.csv')\n",
    "submission_['target'] = pred.mean(axis=1)\n",
    "submission_.to_csv('santander_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i]>0.5:\n",
    "        print(y_pred[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing results to csv file\n",
    "\n",
    "import csv\n",
    "    \n",
    "with open('santander_submission.csv', 'w') as csvFile:\n",
    "    csvFile.write('ID_code' + ',' + 'target' + '\\n')\n",
    "    for row in range(len(y_pred_test)):\n",
    "         csvFile.write('test_' + str(row) + ',' + str(y_pred_test[row]) + '\\n')\n",
    "csvFile.close()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
